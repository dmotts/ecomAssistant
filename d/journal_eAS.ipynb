{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning and Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for ColBERT\n",
    "\n",
    "### Searching ColBERTv2\n",
    "- This will be the main mechanism for the RAG portion of this project, and involves finding relevant data within an indexed database given a user query.\n",
    "- Development of search on CPU is possible given a preindexed database, and will be developed before GPU search due to versitility of CPU-based programs. \n",
    "\n",
    "### Indexing ColBERTv2\n",
    "- Indexing requires a list of passage strings to be fed to the indexer.\n",
    "- The index file contained in the experiment folder can then be saved and used with the Searcher class at any point.\n",
    "- Indexing a dataset \n",
    "\n",
    "### Training ColBERTv2\n",
    "- ColBERTv2 is trained on the MS MARCO Passage Ranking dataset. \n",
    "- Training requires a JSONL triples file with a [qid, pid+, pid-] list per line. The query IDs and passage IDs correspond to the specified queries.tsv and collection.tsv files respectively.\n",
    "    - pid+: positive_passage_id\n",
    "    - pid-: negative_passage_id\n",
    "- Remember that Queries and Collections must be formatted as TSV files:\n",
    "    - Queries: each line is qid \\t query text.\n",
    "    - Collection: each line is pid \\t passage text.\n",
    "- So setting up a triples file entails pairing a query with a corresponding positive and negative passage such that:\n",
    "\t-\tPositive Passage: A passage that correctly and relevantly answers the query.\n",
    "\t-\tNegative Passage: A passage that is either irrelevant or less relevant to the query compared to the positive passage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for Data Scraper\n",
    "\n",
    "### Selenium Docker Image\n",
    "- The datascraper needs Selenium and Chrome drivers to by installed to run. Creating a docker container for this will be difficult, but extremely useful, as installing Chrome drivers on a machine that doesn't have them is a less than trivial issue for setting up this program.\n",
    "- [Docker Selenium Standalone Chrome Image Documentation](https://hub.docker.com/r/selenium/standalone-chrome)\n",
    "\n",
    "\n",
    "### Other\n",
    "- Ensure all data scraped is of a specified currency. i.e. US currency only. This may involve specifying the region items are found in the search. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
